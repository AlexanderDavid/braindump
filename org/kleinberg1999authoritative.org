#+setupfile:./hugo_setup.org
#+TITLE: Authoritative sources in a hyperlinked environment
#+ROAM_KEY: cite:kleinberg1999authoritative
#+ROAM_TAGS: Paper "Network Science"

- tags :: [[file:20200825162306-network_science.org][Network Science]]

* Notes
  :PROPERTIES:
  :Custom_ID: kleinberg1999authoritative
  :URL:
  :AUTHOR: Kleinberg, J. M.
  :NOTER_DOCUMENT: /home/alex/Dropbox/notes/papers/kleinberg1999authoritative.pdf
  :NOTER_PAGE:
  :END:
** Initial Thoughs
:PROPERTIES:
:NOTER_PAGE: 2
:END:
- Could a node purposefully hide links to well cited souces so that it could be considered a hub and then prominently display the links to fake sites? Maybe some sort of weight on a link based on its prominance?
- The space complexity of this is $O(2n)$ and the time complexity cannot be good either
- Is there a way to say some authority page is more important than another hub page? Which one do you return to the user.
- Could some pages earn hub status through other means? Should well respected educational resources be hubbier than a site that has just popped up even though they both share the same links? They could have different content.
- How do you turn a search query into a matching hub or authority. It is one thing to rank them but another to return them as a search query (the authors primary application of this algorithm)
** Testing on a Dataset that would make the algorithm look better
:PROPERTIES:
:NOTER_PAGE: (4 . 0.5028432168968319)
:END:
It seems that by selecting the /focused subgraphs/ the author is selecting a graph that would work with his data. I would like to see it applied a larger, random scale. It could be that a sufficiently large dataset looks the same as this small one though.
** This is pretty much already the set of pages you want to return
:PROPERTIES:
:NOTER_PAGE: (5 . 0.7627944760357434)
:END:
If they are using this algorithm on a subgraph that just contains relevent pages then any ranking method may give good results
** Would be tough to find
:PROPERTIES:
:NOTER_PAGE: (7 . 0.32493907392363935)
:END:
If the links are one directional on the WWW it is hard to find this $\Gamma^{-}(p)$ because the WWW has no concept of /backlinks/
** Unlinked Nodes
:PROPERTIES:
:NOTER_PAGE: 12
:END:
Some nodes may be the best ever but if they are not connected at all to the first 200 search results then they will not be found
** Bias from searching for notable pages with the same search engine that defines the root
:PROPERTIES:
:NOTER_PAGE: (13 . 0.5515840779853778)
:END:
The authors view of "good" authorities could be biased in the sense that the algorithm would be starting from the same set of pages that the author uses. If the author is searching for these pages using AltaVista and so is the algorithm then it makes sense that the set of pages they both consider to be "authoratitative" are similar.
* Summary
Kleinberg's paper /Authoritative Sources in a Hyperlinked Environment/ is a seminal paper in the domain of node centrality ranking. This paper gives an explanation of a node centrality algorithm based on the idea of information hubs and authorities as they relate to the world wide web (WWW). The paper makes the assumption that the WWW is comprised of two main classes of sites. The first class, hubs, are sites dedicated to collecting links to other sites which contain information about a certain topic. The second class of sites, authorities, are sites that contain the information about a certain topic.

Each page, $p$, has a non-negative /authority weight/, $x^{\langle p \rangle}$, and a non-negative /hub weight/, $y^{\langle p \rangle}$. The pages with larger $x$ values are seen as better authorities and the pages with the larger $y$ values are seen as better hubs. These two values are said to have a /mutually reinforcing relationship/ where a good hub points to many good authorities and vice versa. The values of the weights are normalized so the invariant $\Sigma_{p \in S_\sigma}(x^{\langle p \rangle})^2 = \Sigma_{p \in S_\sigma}(y^{\langle p \rangle})^2 = 1$, where $S_\sigma$ is the set of pages to assign weights to, is always true. These weights can be calculated iteratively by applying the following two equations alternatively until the $x$ and $y$ vectors converge to fixed vectors $x^*$ and $y^*$ respectively.

\begin{equation}
    x^{\langle p \rangle} \leftarrow \Sigma_{q:(q, p) \in E} y^{\langle q \rangle}
\end{equation}

\begin{equation}
    y^{\langle p \rangle} \leftarrow \Sigma_{q:(q, p) \in E} x^{\langle q \rangle}
\end{equation}

The paper also offers a proof that the principle eigenvectors of $A^TA$ and $AA^T$ are equal to $x^*$ and $y^*$ respectively.

The nodes that will be ranked are generated by starting with a small graph, $R_\sigma$, containing all the pages returned by some search query $\sigma$ on the text-based search engine AltaVista. A larger graph, $S_\sigma$, is built by slowly expanding $R_\sigma$ with all of the pages that link to and are linked to on each page $p \in R_\sigma$. The nodes within $S_\sigma$ are then assigned hub and authority weights, and the $n$ pages with the largest authority and hub weights are returned by this algorithm.

The paper then goes on to describe similar centrality ideas that exist within the domain of search ranking, sociology, and bibliometrics. Kleinberg then explains some extensions to this algorithm including the ability return multiple densely interconnected hubs and authorities by utilizing the non-principle eigenvectors of $A^TA$ and $AA^T$ and the ability to generalize a given query when not enough search results are found. The conclusions given in the penultimate section are quite convincing with an extension of the hub and authority algorithm being considered a better repository of information than human curated lists a majority of the time.

* Issues
The following subsections outline a few issues I found while reading this paper.
** Issue 1: Malicious Actors
Search Engine Optimization (SEO) is an integral and lucrative field in internet marketing. Companies are constantly tweaking their website in such a way as to boost their ranking in the list of websites returned by certain queries. This algorithm would allow malicious companies to profit on the links they include in their sites. Sites could get into the top 200 ranking on AltaVista and pose as a good hub while actually just charging companies for links to their sites. They could even include links to other prominent authorities in order to boost their hub score while showing links to less prestigious authorities with more emphasis and towards the top of the page. One way to solve this problem is to include a weight between different websites that is equal to the prominence with which the links are shown. This could be scaled so that only a few of the highest visibility links counted from any given hub.
** Issue 2: Which Eigenvector Most Represents the Query
Section 6 of the paper mentions that the non-principle eigenvectors of $A^TA$ and $AA^T$ can yield similar hub and authority rankings for different clusters of information. The example given in the paper is the search term Jaguar returning clusters for the NFL team, the car, and the entertainment system. The presents the problem of a further choice of which cluster of hubs and authorities to present the user with. The main point of sorting the pages within WWW searches is to distill the vast amount of results that can be returned to a user. If the user has to check through multiple different information homonym clusters this may dissuade them from using this service. A solution to this may be to develop a way to rank these different clusters not based upon their denseness but based upon their relevance to the original text search.
** Issue 3: Lack of Backlinks on the Internet
The internet is a series of unidirectional links between websites. To calculate the set of sites that point to some page a web scraper would have to check every single site on internet. Because the algorithm for calculating $S_\sigma$ (seen on page 7) requires each page $p$ to calculate $\Gamma^{-}(p)$, or the set of pages with links to $p$, this requires either a massive amount of storage to store the links between all pages on the internet or a massive amount of computation to find this set of all pages that point to $p$. The computational cost could be reduced by limiting the search to other pages within $R_\sigma$ however this could produce a $S_\sigma$ that is not much different than $R_\sigma$ and reduce the efficacy of the resulting set of authorities.
