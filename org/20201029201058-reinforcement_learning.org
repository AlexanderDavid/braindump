#+setupfile:./hugo_setup.org
#+title: Reinforcement Learning
#+ROAM_TAGS: AI

- tags :: [[file:20200903164633-artificial_intelligence.org][Artificial Intelligence]]

* Definition
- [[file:20201029201035-markov_decision_process.org][Markov Decision Process]] but we don't know $P$ and $R$
* Framework
At every timestep $t$ the agent picks an action $a_t$ and the environment changes to a new state $s_t$ and the agent is given some reward $r_t$
[[file:images/2020-09-15_17-13-25_screenshot.png]]
* Model-Based Learning
[[file:images/2020-09-17_17-13-20_screenshot.png]]
** Model-Based Monte Carlo
1. Learn empirical MDP using Monte Carlo simulation
   a. Episodic data: $s_0, a_0, r_1, s_1, a_1, ..., s_T$
   b. Estimate transitions and rewards
        [[file:images/2020-09-15_17-19-38_screenshot.png]]
   c. Discover each $\hat{R}$ when we experience $(s, a, s')$
2. Estimates converge to the truth
3. Solve using value/policy iteration
* Model-Free Learning
[[file:images/2020-09-17_17-14-56_screenshot.png]]

** Example
-
* Q-Learning
- Used to compute $V^*$, $Q^*$, and $\pi^*$ in a model free, unknown MDP
** Active RL
- Full reinforcement learning
- Learner makes choices
- Exploration vs Exploitation
** Tabular Q-Learning
- Sample-based Q-value iteration
    [[file:images/2020-09-17_17-26-24_screenshot.png]]
- Learn Q-values as you go
  + Recieve a sample $(s, a, r, s')$
  + Consider previous estimate $\hat{Q}(s, a)$
  + Consider your new sample estimate
    - target $= R(s,a,s')+\gamma max_{a'}\hat{Q}(s', a')$
  + Incorperate the new estimate into a running average using some learning rate $\alpha$
    - $\hat{Q}(s, a) \leftarrow (1 - a)\hat{Q}(s, a) + \alpha\text{[target]}$
** Properties
- Q-learning converges to optimal policy, even if you're acting suboptimally if the policy visits every path
- This is called off-policy learning
  + On policy: Learn values of the policy used to generate samples
  + Off policy: Learn the value of another policy
- Caveats
  + Lot of exploration
  + Make the learning rate small enough evantually
  + Basically, in the limit, it doesn't matter how you select actions!
** Exploration vs Exploitation
- All states should be visited infinitely often
- Multi-arm bandit
*** How to sample actions?
- Choose random actions?
  + You need to visit a lot of cells to get enough data of optimal paths
- Choose action greedily?
  + Never try the other paths that could be higher value?
- Answer: start by random and transition to greedy to converge
- Epsilon-greedy
  + With a (small) probability $\epsilon$, act randomly
  + With a (large) probability $1-\epsilon$, act on current policy
